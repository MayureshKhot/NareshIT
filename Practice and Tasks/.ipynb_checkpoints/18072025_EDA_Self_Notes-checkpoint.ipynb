{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810a7585-e226-4f2c-8995-c8721303df59",
   "metadata": {},
   "source": [
    "# Feature Engineering (Most Important-back bone of DS)\n",
    "- $By Krish Naik$\n",
    "- 30% of the ML/Data Sciecne work is Feature Engineering.\n",
    "\n",
    "### Step1: EDA\n",
    "- As soon as we get the raw data, we start doing the analysis:\n",
    "  1. How many numerical features are there (histogram, etc.)\n",
    "  2. How many categorical features are there\n",
    "  3. Is there any missing value? (by visualizing the graphs)\n",
    "  4. Outliers are there? (Box plot)\n",
    "  5. Do raw data needs cleaning or not? (isnull(), drop())\n",
    "\n",
    "### Step2: Handling the missing values\n",
    "- Using mean, median, mode and other ways are there.\n",
    "- You can replace some features as per the requirements.\n",
    "\n",
    "### Step3: Handling Imbalanced dataset\n",
    "- A dataset where the classes are not represented equally. For example, in a binary classification problem:\n",
    "\n",
    "    - Positive class: 5% of data\n",
    "\n",
    "    - Negative class: 95% of data\n",
    "\n",
    "- This imbalance can lead to biased models that predict only the majority class because accuracy becomes misleading.\n",
    "\n",
    "### Step4: Treating the outliers.\n",
    "- Here’s a simple approach to deal with outliers:\n",
    "\n",
    "    - Detect them\n",
    "\n",
    "    - Use statistical methods like:\n",
    "\n",
    "        - Z-score: Values with z > 3 (standard deviations away)\n",
    "\n",
    "        - IQR (Interquartile Range): Values below Q1 - 1.5IQR or above Q3 + 1.5IQR\n",
    "        \n",
    "    - Or visualize using boxplots and scatter plots.\n",
    "        \n",
    "        - Decide what to do:\n",
    "        \n",
    "        - Remove them if they are errors or not relevant.\n",
    "        \n",
    "        - Cap them (Winsorizing) to a maximum/minimum threshold.\n",
    "        \n",
    "- Transform them using log, square root, or other transformations to reduce their impact.\n",
    "\n",
    "- Keep them if they are genuine important variations (e.g. extremely high income customers).\n",
    "\n",
    "### Step5: Scaling the data\n",
    "- Standardization, Normalization, etc.\n",
    "- Definition\n",
    "- Why?\n",
    "- How?\n",
    "- When?\n",
    "- When not?\n",
    "\n",
    "### Step6: Converting the categorical features into Numerical Features (Most Important)\n",
    "- Why?\n",
    "- How?\n",
    "- When?\n",
    "- When not?\n",
    "\n",
    "#### Raw Data has a lot of problems, so Feature engineering cleans the data and then the data is passed to an ML model.\n",
    "\n",
    "## After FE, comes Feature Selections.\n",
    "\n",
    "##### Step 1: Corelation\n",
    "##### Step 2: K Neighbours\n",
    "##### Step 3: Chaisquare\n",
    "##### Step 4: Feature Importance\n",
    "##### Step 5: Generic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f279353-a642-40ac-9667-efd9ac40f331",
   "metadata": {},
   "source": [
    "# Feature Engineering (Backbone of Data Science)\n",
    "_By Krish Naik_\n",
    "\n",
    "✔️ **30% of ML/Data Science work is Feature Engineering**\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: EDA (Exploratory Data Analysis)\n",
    "\n",
    "As soon as you get raw data, analyze:\n",
    "\n",
    "1. **Numerical features** – check with histograms\n",
    "2. **Categorical features** – identify counts and levels\n",
    "3. **Missing values** – visualize with heatmaps or bar graphs\n",
    "4. **Outliers** – detect with box plots or scatter plots\n",
    "5. **Data cleaning needed?** – use `isnull()`, `drop()` to inspect\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Handling Missing Values\n",
    "\n",
    "- Replace using **mean, median, or mode**\n",
    "- Choose based on feature type and distribution\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Handling Imbalanced Dataset\n",
    "\n",
    "- Occurs when classes are **not equally represented**  \n",
    "  _(e.g. Positive: 5%, Negative: 95%)_\n",
    "- Leads to **biased models** predicting majority class only\n",
    "- Techniques include:\n",
    "  - **Oversampling**\n",
    "  - **Undersampling**\n",
    "  - **SMOTE**\n",
    "  - **Class weights**\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Treating Outliers\n",
    "\n",
    "1. **Detect them:**\n",
    "   - **Z-score**: z > 3\n",
    "   - **IQR**: Values < Q1 - 1.5*IQR or > Q3 + 1.5*IQR\n",
    "2. **Visualize** with boxplots or scatter plots\n",
    "3. **Handle them:**\n",
    "   - **Remove** if errors or irrelevant\n",
    "   - **Cap (Winsorize)** to thresholds\n",
    "   - **Transform** using log, square root, etc.\n",
    "   - **Keep** if genuine important variations _(e.g. extremely high income customers)_\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Scaling the Data\n",
    "\n",
    "- **Standardization** (mean = 0, std = 1)\n",
    "- **Normalization** (scale between 0 and 1)\n",
    "\n",
    "✔️ **Why?** Bring features to similar scales  \n",
    "✔️ **How?** Using `StandardScaler`, `MinMaxScaler`  \n",
    "✔️ **When?** Before distance-based or gradient descent-based models  \n",
    "✔️ **When not?** Tree-based models (not mandatory but can help)\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Converting Categorical to Numerical\n",
    "\n",
    "✔️ **Why?** ML models require numerical input  \n",
    "✔️ **How?** One-hot encoding, label encoding, target encoding  \n",
    "✔️ **When?** Before model training  \n",
    "✔️ **When not?** Avoid high-cardinality one-hot encoding unless necessary\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** Raw data has multiple issues. **Feature engineering cleans and transforms data for ML models.**\n",
    "\n",
    "---\n",
    "\n",
    "# After Feature Engineering: Feature Selection\n",
    "\n",
    "1. **Correlation Analysis** – remove highly correlated features\n",
    "2. **K-Nearest Neighbours** – for feature relevance\n",
    "3. **Chi-square test** – for categorical vs categorical relationships\n",
    "4. **Feature Importance** – using tree models\n",
    "5. **Genetic Algorithms** – for advanced selection\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fba483-1b7d-4182-a881-1e39a5ac6ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
